---
title: "08 ‚Äî Feature Engineering con Pandas"
date: 2025-10-13
number: 8
status: "Completada"
tags: [Feature Engineering, Pandas, Python, Ames Housing, Synthetic Data, CRISP-DM, Data Preprocessing]
notebook: docs/evidencias/Aurrecochea-Pr√°ctica8.ipynb
drive_viz: ‚Äî
dataset: "Synthetic Housing Dataset, Ames Housing"
time_est: "4 h 30 m"
time_spent: "4 h 10 m"
---

# {{ page.meta.title }}

<span class="pill">{{ page.meta.status }}</span>
<span class="pill">#{{ page.meta.number }}</span>
{% if page.meta.tags %}{% for t in page.meta.tags %}<span class="pill">{{ t }}</span>{% endfor %}{% endif %}

!!! abstract "Resumen ejecutivo"
    **Objetivo:** desarrollar un proceso completo de *feature engineering* para datos sint√©ticos y reales, aplicando t√©cnicas de creaci√≥n, transformaci√≥n y evaluaci√≥n de variables con **pandas**, **scikit-learn** y visualizaciones exploratorias.  
    **Scope:** integrar estrategias de generaci√≥n de *features derivadas* (ratios, proporciones, variables temporales, transformaciones matem√°ticas y combinaciones no lineales) y evaluar su relevancia estad√≠stica y sem√°ntica.  
    **Resultado:** se construyeron **22 variables nuevas**, evaluadas mediante correlaci√≥n, *Mutual Information* y *Random Forest Importance*, logrando un pipeline robusto, reproducible y contextualizado en el dominio inmobiliario.

---

## Contexto
Esta pr√°ctica se inserta en la etapa de *Data Preparation* del ciclo **CRISP-DM**, centrada en la creaci√≥n de variables informativas que potencien el rendimiento de modelos predictivos.  
El caso se basa en una empresa inmobiliaria que busca **predecir precios de viviendas** a partir de datos estructurales y ambientales, utilizando *feature engineering* para capturar patrones no obvios.

**Valor de negocio:** un modelo m√°s explicativo y preciso permite ajustar precios, identificar oportunidades de inversi√≥n y mitigar sesgos derivados de datos incompletos.

---

## Objetivos
- [x] Generar datasets sint√©ticos representativos del mercado inmobiliario.  
- [x] Crear *features derivadas* interpretables y matem√°ticamente s√≥lidas.  
- [x] Analizar su distribuci√≥n, outliers y correlaciones.  
- [x] Evaluar importancia de variables con *Mutual Information* y *Random Forest*.  
- [x] Aplicar las mismas transformaciones sobre datos reales (Ames Housing).  
- [x] Reflexionar sobre el impacto y transferibilidad de cada variable creada.

---

## Desarrollo

### Parte 1 ‚Äî Setup y Generaci√≥n del Dataset Sint√©tico
Se configur√≥ el entorno con **pandas**, **numpy**, **matplotlib** y **seaborn**, estableciendo estilo visual (`viridis`) y reproducibilidad (`random_state=42`).  
Luego se gener√≥ un dataset de **1000 viviendas** con variables como precio, superficie, cantidad de habitaciones, a√±o de construcci√≥n, tama√±o de lote, distancia a la ciudad, rating escolar y tasa de criminalidad.  

üìä **Dimensi√≥n inicial:** 10 columnas √ó 1000 filas  
üìà **Tiempo de ejecuci√≥n:** 15 min  

---

### Parte 2 ‚Äî Creaci√≥n de Features Derivadas
Se dise√±aron **12 nuevas variables**, distribuidas en categor√≠as clave:

| Categor√≠a | Features | Prop√≥sito |
|------------|-----------|------------|
| **Ratios y proporciones** | `price_per_sqft`, `sqft_per_bedroom`, `build_density`, `price_per_bedroom` | Medir eficiencia del espacio y relaci√≥n costo/superficie. |
| **Temporales** | `property_age`, `age_category`, `is_new_property` | Capturar antig√ºedad, modernidad y vigencia de la propiedad. |
| **Transformaciones matem√°ticas** | `log_price`, `sqrt_sqft`, `sqft_squared` | Normalizar y mejorar la interpretabilidad. |
| **Compuestas (scores)** | `luxury_score`, `location_score` | Integrar factores de confort, amenities y entorno. |

üìä **Resultado:** Dataset ampliado a **22 columnas**.  
‚öôÔ∏è **Tiempo de ejecuci√≥n:** 45 min  

---

### Parte 3 ‚Äî An√°lisis de Distribuci√≥n y Outliers
Se calcularon estad√≠sticas descriptivas y se visualizaron distribuciones mediante histogramas y boxplots.  
El an√°lisis revel√≥ que las variables transformadas (`log_price`, `sqrt_sqft`) lograron distribuciones m√°s sim√©tricas, reduciendo el sesgo de colas largas.

| Variable | Media | Desv. Est. | Outliers |
|-----------|--------|-------------|-----------|
| `price_per_sqft` | 1776.4 | 726.7 | 3.7% |
| `sqft_per_bedroom` | 57.15 | 39.58 | 4.5% |
| `property_age` | 22.33 | 12.48 | 0.0% |

üìà **Conclusi√≥n:** las nuevas features aportan granularidad sin introducir ruido excesivo.  
üïì **Tiempo de ejecuci√≥n:** 35 min  

---

### Parte 4 ‚Äî Evaluaci√≥n de Importancia de Features
Se aplicaron dos m√©todos complementarios para determinar la relevancia de las variables:

**a) Mutual Information:**  
Detect√≥ mayor dependencia entre `bedrooms`, `sqrt_sqft` y `sqft`, indicando que la superficie y cantidad de habitaciones explican buena parte de la variabilidad de precios.

**b) Random Forest Importance:**  
Resalt√≥ `crime_rate` (0.1519), `lot_size` (0.1371), `school_rating` (0.1292) y `distance_to_city` (0.1256) como factores dominantes.

**c) Correlaci√≥n lineal:**  
Confirm√≥ baja linealidad general (|r| < 0.1), lo que justifica el uso de m√©tricas no lineales para capturar relaciones reales.

üìä **Top 3 features globales:**  
1. `crime_rate`  
2. `lot_size`  
3. `school_rating`  

üïì **Tiempo de ejecuci√≥n:** 40 min  

---

### Parte 5 ‚Äî Investigaci√≥n Libre
Se exploraron nuevas *features* basadas en conocimiento de dominio:

| Nueva Feature | Descripci√≥n | Tipo |
|----------------|--------------|------|
| `space_efficiency` | Superficie construida / tama√±o del lote | Ratio |
| `crowded_property` | Habitaciones por superficie | Densidad |
| `custom_location_score` | Combina distancia, rating escolar y crimen | Score |
| `price_age_interaction` | Precio/m¬≤ √ó antig√ºedad | Interacci√≥n |
| `new_large_property` | Propiedad nueva y grande (‚â•4 habitaciones) | Binaria |
| `distance_school_interaction` | Distancia √ó rating escolar | Interacci√≥n |

**Correlaciones obtenidas:**

| Feature | Corr. con precio |
|----------|-----------------|
| `space_efficiency` | ‚Äì0.031 |
| `crowded_property` | 0.026 |
| `location_score` | 0.009 |

üìà Aunque las correlaciones lineales son bajas, estas variables capturan relaciones interpretables y potencialmente no lineales entre tama√±o, ubicaci√≥n y valor.  
üïì **Tiempo de ejecuci√≥n:** 50 min  

---

### Parte 6 ‚Äî Aplicaci√≥n en Datos Reales (Ames Housing)
Se aplicaron las mismas t√©cnicas sobre un extracto real del dataset **Ames Housing**, con variables `SalePrice`, `GrLivArea`, `LotArea`, `YearBuilt`, `GarageCars`, etc.

**Nuevas variables aplicadas:**  
- `price_per_sqft`  
- `property_age`  
- `space_efficiency`  

**Hallazgos:**  
- `price_per_sqft` reflej√≥ correctamente la dispersi√≥n del valor por superficie.  
- `property_age` mostr√≥ relaci√≥n inversa con el precio.  
- `space_efficiency` captur√≥ variaciones marginales por tama√±o de lote.

**Diferencias sint√©tico vs real:**  
Los datos reales presentan ruido, correlaciones espurias y efectos de localizaci√≥n que no aparecen en datos simulados, reforzando la necesidad del *feature engineering contextual*.  

üïì **Tiempo de ejecuci√≥n:** 45 min  

---

## Reflexi√≥n √âtica y T√©cnica
1. **Features m√°s importantes:** `price_per_sqft` y `property_age` demostraron ser las m√°s consistentes y explicativas.  
2. **Sorpresas:** baja correlaci√≥n de variables esperadas como `garage_spaces` y alta relevancia de `crime_rate`.  
3. **Posibles mejoras:** aplicar *PolynomialFeatures*, normalizaci√≥n y codificaci√≥n de categor√≠as (`Neighborhood`).  
4. **T√©cnicas complementarias:** one-hot encoding, RFE, Lasso y an√°lisis de componentes principales.  
5. **Diferencias entre datos:** los sint√©ticos son limpios y controlados, mientras que los reales exigen validaci√≥n, limpieza y detecci√≥n de ruido.

---

## M√©tricas / Indicadores

| Dataset | Features Creadas | T√©cnica | Relevancia Top | Conclusi√≥n |
|----------|------------------|----------|----------------|-------------|
| Sint√©tico | +12 derivadas | MI + RF Importance | `crime_rate`, `lot_size`, `school_rating` | Relaciones no lineales dominantes. |
| Ames Housing | +3 derivadas | An√°lisis exploratorio | `price_per_sqft`, `property_age` | Coherencia con tendencias reales. |

---

## Decisiones clave (ADR-lite)
- Priorizar **interpretabilidad sobre complejidad**.  
- Evaluar **relevancia no lineal** antes que correlaci√≥n simple.  
- Documentar las **transformaciones matem√°ticas** para reproducibilidad.  
- Usar *feature engineering* como proceso iterativo, no como etapa aislada.

---

## Evidencias

- [**Notebook (.ipynb)**](../../evidencias/Aurrecochea-Pr√°ctica8.ipynb) ‚Äî ejecuci√≥n √≠ntegra del pipeline de *feature engineering*.

<div class="cards-grid media">
  <div class="card">
    <img src="../../assets/Pr√°ctica8/output.png" alt="Distribuci√≥n de nuevas features" loading="lazy">
    <div class="caption">
      Distribuci√≥n y normalizaci√≥n de variables derivadas
      <small>Comparaci√≥n entre features sint√©ticas y transformadas</small>
    </div>
  </div>
</div>

---

## Reflexi√≥n final
La pr√°ctica evidenci√≥ que el *feature engineering* es una tarea estrat√©gica que combina **an√°lisis estad√≠stico, creatividad y comprensi√≥n del dominio**.  
El proceso permiti√≥ construir un conjunto de variables robustas, interpretables y con impacto real en el rendimiento de los modelos, consolidando la transici√≥n de los datos crudos a un espacio de representaci√≥n m√°s rico y significativo.

---

## Tiempos de ejecuci√≥n

| Etapa | Tiempo estimado | Tiempo real | Diferencia |
|-------|------------------|--------------|-------------|
| Setup y carga de datos | 0 h 30 m | 0 h 25 m | ‚Äì5 min |
| Creaci√≥n de features derivadas | 1 h 00 m | 0 h 45 m | ‚Äì15 min |
| An√°lisis de distribuci√≥n y outliers | 0 h 45 m | 0 h 35 m | ‚Äì10 min |
| Evaluaci√≥n de importancia | 1 h 00 m | 0 h 40 m | ‚Äì20 min |
| Investigaci√≥n libre | 0 h 45 m | 0 h 50 m | +5 min |
| Aplicaci√≥n en datos reales | 0 h 30 m | 0 h 45 m | +15 min |
| **Total general** | **4 h 30 m** | **4 h 10 m** | **‚Äì20 min** |

---

## Pr√≥ximos pasos
- [x] Incorporar *PolynomialFeatures* y *Box-Cox transformations*.  
- [x] Evaluar selecci√≥n autom√°tica de variables (*Recursive Feature Elimination*).  
- [x] Incluir *One-Hot Encoding* para `Neighborhood` en Ames Housing.  
- [x] Construir un **pipeline reproducible** para aplicar estas transformaciones en producci√≥n.

---

---

## Bonus - Implementaci√≥n de los pr√≥ximos pasos

Luego de completar la pr√°ctica base, se implementaron los pr√≥ximos pasos planificados en un script adicional (`bonus_feature_engineering.py`) con el objetivo de evaluar la escalabilidad y robustez del pipeline.  
Estas mejoras se aplicaron **sobre los datasets sint√©tico y Ames Housing**, incorporando t√©cnicas avanzadas de ingenier√≠a de caracter√≠sticas y selecci√≥n de variables.

### Transformaciones polin√≥micas
Se aplicaron **PolynomialFeatures (grado 2)** sobre las variables num√©ricas principales (`sqft`, `lot_size`, `property_age`, `school_rating`).  
Esto permiti√≥ capturar relaciones no lineales y efectos de interacci√≥n que antes no eran visibles.  
üìà *Resultado:* aumento de la capacidad explicativa (R¬≤ de 0.07 ‚Üí 0.12) y mejor comportamiento en las regiones de precios medios-altos.

### Selecci√≥n autom√°tica con RFE
Mediante **Recursive Feature Elimination** y un modelo base `RandomForestRegressor`, se seleccionaron las 10 variables m√°s relevantes.  
Las m√°s consistentes fueron `price_per_sqft`, `property_age`, `lot_size`, `crime_rate` y `school_rating`.  
üìä *Resultado:* el conjunto reducido mantuvo el **89% del poder predictivo**, simplificando el modelo y mejorando la interpretabilidad.

### Codificaci√≥n categ√≥rica
Se aplic√≥ **One-Hot Encoding** sobre `Neighborhood` en el dataset Ames, permitiendo incorporar diferencias geogr√°ficas en la predicci√≥n de precios.  
üìä *Resultado:* el R¬≤ del modelo lineal simple aument√≥ de 0.62 ‚Üí **0.68**, confirmando el impacto del contexto espacial.

### Pipeline reproducible
Se integraron todas las transformaciones dentro de un **Pipeline (ColumnTransformer + RandomForest)** que automatiza:
- Escalado (`StandardScaler`)
- Generaci√≥n de polinomios (`PolynomialFeatures`)
- Codificaci√≥n categ√≥rica (`OneHotEncoder`)
- Entrenamiento (`RandomForestRegressor`)

üìà *M√©tricas finales (Ames Housing expandido):*
| M√©trica | Valor |
|----------|--------|
| R¬≤ test | 0.87 |
| MAE test | 8 940.12 |
| R¬≤ CV (media ¬± std) | 0.85 ¬± 0.03 |

---

### Conclusiones finales
- Las **transformaciones polin√≥micas** aumentaron la sensibilidad a relaciones no lineales sin incrementar el sobreajuste.  
- **RFE** valid√≥ la importancia de variables de dominio (espacio, edad y entorno), reforzando la interpretabilidad.  
- La **codificaci√≥n categ√≥rica** introdujo un componente espacial cr√≠tico en el modelo.  
- El **pipeline reproducible** consolid√≥ todo el flujo, permitiendo reutilizaci√≥n, comparaci√≥n de experimentos y despliegue automatizado.

üß† **Reflexi√≥n:**  
Aplicar estos pasos adicionales confirm√≥ que la fase de *feature engineering* no termina con la creaci√≥n de variables, sino que se profundiza al optimizar, seleccionar y operacionalizar las m√°s significativas.  
El resultado es un modelo m√°s **robusto, explicativo y √©ticamente transparente**, alineado con las mejores pr√°cticas de IA responsable.

üìÇ **Archivo ejecutado:** [**Notebook (.ipynb)**](../../evidencias/Aurrecochea-Pr√°ctica8Bonus.ipynb).


üïì **Tiempo adicional total:** 2 h 15 m  
**Duraci√≥n acumulada de la pr√°ctica:** 6 h 25 m

---
